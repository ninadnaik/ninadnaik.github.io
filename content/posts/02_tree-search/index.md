---
title: "Intelligence as Tree Search"
slug: "intelligence-as-tree-search"
date: 2025-09-04
tags: ["AI", "Intelligence", "Search", "Evolution"]
author: "Ninad Naik"
description: "Intelligence is massively parallel tree search guided by memory and logic. From evolution's glacial updates to humanity's network-speed learning, we trace how the speed of belief updates determines intelligence levels — and why AGI will emerge from millions of specialized agents, not a single superintelligence."
---

# Intelligence as Tree Search
## Memory, Logic, and Compute at Civilization Scale

> The divine feeling of intelligence — the flash of insight, the proof, the invention — is just the end point of billions of failed branches. It is the tip of a tree we've been pruning for eons.

## Setting the Stage

In our [previous](/posts/what-is-intelligence/) exploration, we proposed that every mind exists somewhere in a three-dimensional space defined by memory (what we know), compute (how we explore possibilities), and logic (how we compress and transfer patterns). These aren't independent forces but parts of an endless cycle: memory guides computation, computation reveals patterns, logic crystallizes what works, and the results flow back into memory for the next round.

Now we're going to push this framework to its breaking point and see what emerges. I want to convince you of two ideas that might initially seem outrageous:

First, that correlation can effectively function as causation - not philosophically, but practically — if you search widely enough, intervene actively enough, and update your beliefs ruthlessly enough based on what you find.

Second, that artificial general intelligence won't arrive as a monolithic superintelligence invented in a single lab. Instead, it will emerge from millions of diverse agents coordinating their search, exchanging results, and updating their collective understanding at unprecedented speed.

The thread connecting all of this? Intelligence is massively parallel tree search, guided by memory and logic, with the crucial ability to update beliefs as quickly and cleanly as possible based on feedback from reality. And here's the key: the speed of those updates determines the level of intelligence we achieve.

## Evolution: Updates Once Per Generation

Evolution is the original search algorithm, and it's been running for nearly four billion years. It branches through mutation and recombination, evaluates through survival and reproduction, then prunes ruthlessly. DNA serves as its memory — an ultra-compressed ledger of what worked and what catastrophically didn't.

Consider the Pacific salmon. Every year, millions of these fish navigate thousands of miles of open ocean to return to the exact stream where they were born. As juveniles, salmon imprint on the unique chemical signature of their natal stream, likely combined with magnetic field cues. Years later, they return using these stored patterns, adjusting their path even when conditions shift.

This isn't just memory playing back like a recording. It's memory (genetic instructions for imprinting), logic (if-then rules for following gradients), and compute (real-time sensing and correction) all working together. The salmon's ancestors paid for this capability with countless failed attempts — fish that swam the wrong way and never reproduced, their navigational strategies pruned from the genetic tree.

Evolution demonstrates something crucial: memory-heavy intelligence can yield startling capability when the evaluation loop is vast enough and harsh enough. But evolution's logic updates slowly — each generation tests variations, and only successful logical rules (behavioral patterns, chemical pathways, neural structures) survive to the next generation. This glacial pace of updating both memory and logic is evolution's fundamental constraint.

## Learning Animals: Updates Within a Lifetime

The leap from evolved behaviors to learned behaviors marks one of the great transitions in intelligence. It's the difference between waiting generations to update your strategies and updating them within a single lifetime.

Take birdsong. A young songbird doesn't hatch knowing its species' song. Instead, it listens to adult tutors, stores a template in memory, then practices through babbling — noisy attempts that gradually converge on the target. The bird's brain contains dedicated circuits for this learning process. Isolate the bird from adult song or deafen it so it can't hear its own practice, and learning fails. This is search with a teacher, compute guided by memory, with logic extracting the patterns that matter.

Corvids — crows and ravens — go further. When a crow encounters a puzzle box with food inside, watch carefully and you can almost see it thinking, testing solutions mentally before acting. Present a crow with a tube of water containing a floating treat just out of reach, and it will drop stones to raise the level. But here's what's impressive: they select heavy stones over light ones, solid objects over hollow ones. They've developed logic — causal rules about weight and displacement — through repeated experimentation.

The key advance? These animals can update their logical understanding multiple times within their lifetime. A crow that fails to get food doesn't just try randomly; it updates its logic about what might work. It's eliminating branches of the search tree based on accumulated evidence. This acceleration in update speed — from generational to within-lifetime — creates a qualitative leap in intelligence.

## Humanity: Updates at Network Speed

Humans inherit the same biological substrate as other learning animals. What we add transforms intelligence from an individual property to a collective phenomenon: the ability to update our collective priors — our shared logic about how the world works — at the speed of communication.

We've turned our species into a civilization-scale search engine. Think of each human as a node running experiments, culture as our shared memory bank that preserves what works, and science as our collective logic engine that extracts principles from observations. When someone discovers electricity can flow through wire, that update doesn't die with them — it becomes a new logical rule that propagates through the network instantly.

Consider electricity's journey from curiosity to civilization-transformer. No single person reasoned their way to the electric motors in today's vehicles. Thousands of people ran experiments, building on prior successes. We collectively pruned failed approaches, focused attention on winners, then ran new experiments. Each person brought decades of specialized expertise — itself the product of countless experiments and refinements. Most importantly, each discovery updated our collective logic: Faraday's law became a logical rule that thousands could build upon, Maxwell's equations provided logical frameworks that guided entirely new branches of exploration.

The difference from animals isn't just language or tools. It's the speed and scale at which we update our logical understanding. When one lab discovers a new principle, thousands of other labs can apply that logic immediately. We don't just learn within our lifetime — we continuously refine our collective logic, preserving every useful inference rule and discarding what doesn't generalize.

But here's what matters most: human intelligence emerges from the network, not from any individual node. I can't build a pencil from scratch. No one can. Yet pencils exist because our distributed intelligence coordinates specialized knowledge across thousands of people.

## Current AI: Powerful Compute, Glacial Updates

Modern AI, particularly large language models, represents a fascinating extreme: massive computation with pattern-matching prowess, but update speeds reminiscent of evolution rather than human culture.

These models write poetry, translate languages, and generate code. They even exhibit what looks like logic — following rules, making inferences, solving problems. But they have a crippling limitation: they only update during training. Once deployed, they're frozen. Their logic, extracted from training data, cannot evolve based on new experiences. Every interaction starts fresh, like meeting someone with perfect capabilities but complete amnesia about your previous conversations.

Even recent "memory" features are clever engineering workarounds, not native learning. The model's underlying logic — its understanding of causal relationships and inference rules — remains frozen.[^1]

More fundamentally, current AI can't run the rapid experimentation-and-update cycle that defines intelligence. A model trained on data from 2023 doesn't know what happened in 2024 unless retrained from scratch — a process costing millions of dollars and months of time. Compare this to a human who can update their logical understanding instantly upon reading a news article or observing an unexpected outcome.

The issue isn't that models memorize; memorization is a feature of intelligence, not a bug. Evolution memorizes in DNA, culture in books, individuals in synapses. The issue is that current models can't update their logic based on new evidence fast enough. They can't run experiments, observe outcomes, and revise their inference rules. They're frozen at the moment of training, like evolution frozen at a single generation.

## The Agentic Future: Updates at Silicon Speed

Here's my prediction: artificial general intelligence won't arrive as a single model that knows everything. Instead, it will emerge from millions of specialized agents, each updating continuously based on their experiments and observations.

So what exactly is an agent? Not just a chatbot or an API call to a model. An agent is a persistent, learning entity with five essential properties:

First, **state persistence**: It maintains memory across interactions; not just conversation context, but actual learned knowledge that accumulates over time.

Second, **continuous learning**: It updates its beliefs and logical rules based on outcomes. If it hypothesizes that approach X will solve problem Y, tries it, and fails, it updates its logic to exclude that branch in future searches.

Third, **autonomous execution**: It can take actions, observe results, and decide next steps without human intervention for extended periods. This requires judgement: knowing when to proceed, when to explore alternatives, and when to escalate to humans.

Fourth, **bounded scope**: It specializes in a domain narrow enough to enable rapid updates. You can't update a trillion parameters nightly, but you can update a billion. More importantly, specialized scope means faster logical refinement; an agent focused on protein folding can quickly update its rules about molecular interactions without having to maintain logic about Renaissance poetry.

Fifth, **interoperability**: It can call tools, coordinate with other agents, and contribute to shared knowledge pools. This is how individual logical discoveries propagate through the network.

Here's a concrete example: imagine a materials research agent searching for better battery compounds. It starts with a small model (maybe 100M parameters, trainable overnight on a single GPU) that handles natural language I/O and encodes basic chemistry knowledge. When given a research goal, it:

- Queries databases for known compounds with relevant properties (tool-calling)
- Runs computational simulations to generate candidate compounds (modeling)
- Ranks candidates by predicted viability and selects the most promising based on foundational knowledge and queries to specialist data sources (tool-calling, evaluation)
- Collaborates with human researchers or other viable partners to test top candidates in the lab (human-AI partnership, coordination)
- Waits for experimental results, which might take days or weeks (real-world feedback)
- Updates its understanding of structure-property relationships based on what worked and what didn't (learning)
- Shares findings with specialized agents focused on different aspects—synthesis, stability, cost (agent coordination)
- Eliminates entire classes of structures that repeatedly fail (judgement through pruning)

As experimental results arrive over weeks and months, this agent continuously refines its understanding. Failed hypotheses don't just get marked wrong - they strengthen inhibitory pathways that say "avoid this entire class of approaches." Successful patterns create new logical rules that guide future exploration. The agent develops sophisticated judgement not through massive scale but through iterative refinement based on real-world feedback.

Between millions of such agents, we get a vast ecology. Some agents explore wildly, trying new approaches. Others exploit known solutions, refining them. They collaborate, compete, experiment, identify dead ends, and double down on promising branches. Most importantly, they update their logic continuously - not every few months when retrained, but every few minutes as they work.

This is what AGI will look like: not a god in a box, but a civilization in silicon.[^2]

Foundation models won't disappear; they'll become utilities providing basic language understanding and reasoning, like TCP/IP enables the internet without determining what happens at the endpoints.[^3] The real intelligence emerges from the agent layer: millions of specialists, each maintaining their own rapidly-updating memory and logic, coordinating their search through the vast space of possibilities.

## What This Means for AGI

Let me be concrete about what AGI actually means in this framework. It's not a single system that surpasses human intelligence across all domains. Instead, AGI arrives when we can delegate the majority of our daily decision-making to AI agents that learn and update as fast as — or faster than — we do.

Imagine having a 24/7 executive assistant that handles complex decisions, escalating only the most critical ones to you. This isn't just following standard operating procedures — it's exercising judgement. Good judgement comes from rapidly updated logic: knowing that approach A failed yesterday, that client B prefers format C, that similar problems were solved using method D. Each interaction refines the agent's logic about what works, what doesn't, and what needs human attention.

Scale this up: millions of such agents, each specialized yet interconnected. When one agent discovers a new chemical synthesis pathway, its logical insight — not just the result but the reasoning pattern — develops and propagates through the network. Other agents can immediately apply this logic to their own domains, testing whether similar patterns work in their contexts.

This is how we get the explosion in productivity and discovery that AGI promises. Not through a single superintelligence, but through massive parallelization — millions of agents running experiments simultaneously, each updating their piece of the logical framework, with updates propagating at silicon speed rather than biological speed.

The implications are staggering. Today, humanity's collective intelligence is limited by our biological update speed and the number of humans engaged in productive work. Tomorrow, we could have billions of agents, each running thousands of experiments per day, updating their logical understanding in real-time. The tree search that took evolution billions of years, that took human civilization centuries, could happen in decades or years.

## The Critical Requirement: Everyone Must Participate

Here's what keeps me up at night: we can't get complacent because we saw a tweet about some lab discovering a new molecule. That's not AGI — that's a single branch explored in an infinite tree.

Real AGI requires thousands, millions of people managing their own agents, each exploring different branches, discovering novel solutions, and sharing their findings. This isn't a philosophical point about power or equality — it's a practical requirement. Intelligence is a massive tree search exercise, and we need everyone contributing to the search.

Think about what made human intelligence explode: not a single genius brain, but millions of ordinary brains connected in a network, each contributing their small discoveries. The same will be true for artificial intelligence. One lab, no matter how well-funded, can't explore enough branches. One company, no matter how powerful, can't update fast enough.

We need a world where a biologist in Mumbai can spin up agents to explore protein interactions while they sleep. Where a teenager in São Paulo can deploy agents to search for new mathematical proofs. Where a farmer in Kenya can have agents optimizing crop yields based on local conditions. Each person's agents learn from their specific context, update based on their unique experiments, and contribute their logical discoveries to our collective intelligence.

This is why the future of AGI isn't about bigger models — it's about more agents, updating faster, managed by more people. We don't need everyone to have a trillion-parameter model. We need everyone to have their own 100-million parameter model — trainable overnight on a single GPU with today's technology — that updates based on what they learned that day, refining its logic through actual experience rather than just consuming pre-training data.

Of course, we may still need architectural breakthroughs to truly integrate memory, logic, and compute the way biological systems do. The human brain uses sleep to consolidate memories and update its "weights" in ways we don't fully understand. Current architectures keep these capabilities somewhat separate; memory is bolted on, logic emerges implicitly, compute dominates. The next breakthrough might be architectures that natively weave all three together. But even with today's technology, the path forward is clear: smaller, faster-updating personal models beat massive, frozen shared ones.

## The View from Here

Intelligence doesn't feel like search, but it is. The flash of insight, the sudden solution — these feel divine. But they're actually the final leaf on a branch of an enormous search tree that's been growing and getting pruned since the first replicator emerged in primordial soup.

Evolution paid for biological intelligence with countless extinctions — one update per generation. Animals accelerated by updating within lifetimes. Humans exploded by updating at communication speed. Current AI is stuck at evolution's pace — one update per training run.

The next leap is obvious: agents that update continuously, millions of them, exploring every branch in parallel, refining their logic with each experiment. Not a god-model that knows all, but a civilization of learners, each contributing their discoveries to our collective intelligence.

Progress has always been path-dependent hill-climbing, not omniscient calculation. We don't find global optima — we chain local discoveries together. Evolution didn't design the perfect brain; it found what worked. Science doesn't derive truth from first principles; it builds better models through experimentation. And AGI won't be a singular breakthrough — it will emerge from millions of agents running trillions of experiments, updating continuously, sharing instantly.

That's how intelligence evolved. That's how it works in humans. And that's how we'll build it in machines — not through a single magnificent model, but through the ancient algorithm of parallel search and rapid updates, running at silicon speed, with everyone contributing to the search.

The future isn't a lab in San Francisco training an ever-larger model. It's a million people worldwide, each running their own agents, each exploring their own branches, each contributing to a collective intelligence that updates not yearly, not monthly, but continuously.

That's how we got here. That's how we go further.

---

[^1]: Recent approaches to "memory" in AI products—from retrieval-augmented generation to context caching—are impressive engineering but not true learning. They're more like giving someone with amnesia a better notebook, not curing the amnesia itself. The model's core logic about how the world works remains frozen.

[^2]: Consider how a company functions: it's a collection of agents (employees) with specialized roles, each maintaining private memory (expertise) while contributing to institutional memory (documentation, processes). They use tools (software, equipment), follow logical rules (procedures, best practices), and update these rules based on outcomes (post-mortems, retrospectives). The company's intelligence emerges not from any individual but from the orchestration of all these agents. AGI will work the same way, just with silicon agents updating at MHz rather than biological agents updating daily.

[^3]: The parallel to telecommunications is instructive. Phone companies once tried to own everything—the wires, the switches, the phones, the services. Over time, networks became interoperable utilities while value migrated to the edges. The same will happen with AI: foundation models will commoditize into utilities while intelligence emerges from the agent orchestration layer.